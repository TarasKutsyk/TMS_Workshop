{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarasKutsyk/TMS_Workshop/blob/main/TMS_Workshop_Part_2_SAEs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0IUfT02DfFD"
      },
      "source": [
        "# Sparse Autoencoders\n",
        "Part of ML in PL 2025 Workshop: From Superposition to Sparse Autoencoders: Understanding Neural Feature Representations by P. Wielopolski & T. Kutsyk.\n",
        "\n",
        "Based on [ARENA notebook](https://github.com/callummcdougall/ARENA_3.0/blob/main/chapter1_transformer_interp/exercises/part31_superposition_and_saes/1.3.1_Toy_Models_of_Superposition_%26_SAEs_exercises.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cN-eVZGDfFF"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-13-1.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8f1bLQEDfFG"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this section, you'll learn about sparse autoencoders (SAEs) and how they can help resolve superposition problems. You'll train a sparse autoencoder on the toy model setup from the earlier tutorial.\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Understand sparse autoencoders and how they can disentangle features represented in superposition\n",
        "> - Train your own SAE on toy models from earlier sections and visualize the feature reconstruction process\n",
        "\n",
        "\n",
        "Sparse autoencoders represent a promising recent development in mechanistic interpretability, notably explored by Anthropic in their [recent paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html). This approach is currently one of the most active areas of research in the field.\n",
        "\n",
        "In the following exercises, you will:\n",
        "\n",
        "- Build your own sparse autoencoder by implementing its architecture and loss function\n",
        "- Train your SAE on the hidden activations of the `Model` class you defined earlier (note: this differs from Anthropic's approach, which trained SAEs on MLP layer outputs)\n",
        "- Extract features from your trained SAE and verify they match your model's learned features\n",
        "\n",
        "\n",
        "*Short Q&A*\n",
        "\n",
        "<details>\n",
        "<summary>What is an autoencoder, and what is it trained to do?</summary>\n",
        "\n",
        "Autoencoders are a type of neural network which learns efficient encodings / representations of unlabelled data. It is trained to compress the input in some way to a **latent representation**, then map it back into the original input space. It is trained by minimizing the reconstruction loss between the input and the reconstructed input.\n",
        "\n",
        "The \"encoding\" part usually refers to the latent space being lower-dimensional than the input. However, that's not always the case, as we'll see with sparse autoencoders.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/chloeli-15/ARENA_img/main/img/sae-diagram-2.png\" width=\"900\">\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Why is the hidden dimension of our autoencoder larger than the number of activations, when we train an SAE on an MLP layer?</summary>\n",
        "\n",
        "Usually the latent vector is a compressed representation of the input because it's lower-dimensional. However, it can still be a compressed representation even if it's higher dimensional, if we enforce a sparsity constraint on the latent vector (which in some sense reduces its effective dimensionality).\n",
        "\n",
        "As for why we do this specifically for our autoencoder use case, it's because we're trying to recover features from superposition, in cases where there are **more features than neurons**. We're hoping our autoencoder learns an **overcomplete feature basis**.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Why does the L1 penalty encourage sparsity? (This isn't specifically mentioned in this paper, but it's an important thing to understand.)</summary>\n",
        "\n",
        "Unlike $L_2$ penalties, the $L_1$ penalty actually pushes values towards zero. This is a well-known result in statistics, best illustrated below:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/l1-viz.png\" width=\"450\">\n",
        "\n",
        "See [this Google ML page](https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization) for more of an explanation (it also has a nice out-of-context animation!).\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPHU0ZCUDfFH"
      },
      "source": [
        "## Setup (don't read, just run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0jFfrsICDfFH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "chapter = \"chapter1_transformer_interp\"\n",
        "repo = \"ARENA_3.0\"\n",
        "branch = \"main\"\n",
        "\n",
        "# Install dependencies\n",
        "try:\n",
        "    import transformer_lens\n",
        "except:\n",
        "    %pip install einops datasets jaxtyping \"sae-lens>=4.0.0,<5.0.0\" tabulate eindex-callum transformer_lens==2.11.0\n",
        "    %pip install --force-reinstall numpy pandas\n",
        "\n",
        "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
        "root = (\n",
        "    \"/content\"\n",
        "    if IN_COLAB\n",
        "    else \"/root\"\n",
        "    if repo not in os.getcwd()\n",
        "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
        ")\n",
        "\n",
        "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
        "    if not IN_COLAB:\n",
        "        !sudo apt-get install unzip\n",
        "        %pip install jupyter ipython --upgrade\n",
        "\n",
        "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
        "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
        "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
        "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
        "        !rm {root}/{branch}.zip\n",
        "        !rmdir {root}/{repo}-{branch}\n",
        "\n",
        "\n",
        "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
        "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "os.chdir(f\"{root}/{chapter}/exercises\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CYEeCp5DfFI"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Literal\n",
        "\n",
        "import einops\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch as t\n",
        "from IPython.display import HTML, display\n",
        "from jaxtyping import Float\n",
        "from torch import Tensor, nn\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.nn import functional as F\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = t.device(\n",
        "    \"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = \"chapter1_transformer_interp\"\n",
        "section = \"part31_superposition_and_saes\"\n",
        "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
        "exercises_dir = root_dir / chapter / \"exercises\"\n",
        "section_dir = exercises_dir / section\n",
        "\n",
        "import part31_superposition_and_saes.tests as tests\n",
        "import part31_superposition_and_saes.utils as utils\n",
        "from plotly_utils import imshow, line\n",
        "\n",
        "MAIN = __name__ == \"__main__\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q9ev5P1DfFO"
      },
      "source": [
        "### Problem setup\n",
        "\n",
        "Recall the setup of our previous model:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "h &= W x \\\\\n",
        "x' &= \\operatorname{ReLU}(W^T h + b)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "We're going to train our autoencoder to just take in the hidden state activations $h$, map them to a larger (overcomplete) hidden state $z$, then reconstruct the original hidden state $h$ from $z$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "z &= \\operatorname{ReLU}(W_{enc}(h - b_{dec}) + b_{enc}) \\\\\n",
        "h' &= W_{dec}z + b_{dec}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Note the choice to have a different encoder and decoder weight matrix, rather than having them tied - we'll discuss this more later.\n",
        "\n",
        "It's important not to get confused between the autoencoder and model's notation. Remember - the model takes in features $x$, maps them to **lower-dimensional** vectors $h$, and then reconstructs them as $x'$. The autoencoder takes in these hidden states $h$, maps them to a **higher-dimensional but sparse** vector $z$, and then reconstructs them as $h'$. Our hope is that the elements of $z$ correspond to the features of $x$.\n",
        "\n",
        "Another note - the use of $b_{dec}$ here might seem weird, since we're subtracting it at the start then adding it back at the end. The way we're treating this term is as a **centralizing term for the hidden states**. It subtracts some learned mean vector from them so that $W_{enc}$ can act on centralized vectors, and then this term gets added back to the reconstructed hidden states at the end of the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def constant_lr(*_):\n",
        "    return 1.0\n",
        "\n",
        "@dataclass\n",
        "class ToyModelConfig:\n",
        "    n_inst: int\n",
        "    n_features: int = 5\n",
        "    d_hidden: int = 2\n",
        "    n_correlated_pairs: int = 0\n",
        "    n_anticorrelated_pairs: int = 0\n",
        "    feat_mag_distn: Literal[\"unif\", \"normal\"] = \"unif\"\n",
        "\n",
        "\n",
        "class ToyModel(nn.Module):\n",
        "    W: Float[Tensor, \"inst d_hidden feats\"]\n",
        "    b_final: Float[Tensor, \"inst feats\"]\n",
        "\n",
        "    # Our linear map (for a single instance) is x -> ReLU(W.T @ W @ x + b_final)\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cfg: ToyModelConfig,\n",
        "        feature_probability: float | Tensor = 0.01,\n",
        "        importance: float | Tensor = 1.0,\n",
        "        device=device,\n",
        "    ):\n",
        "        super(ToyModel, self).__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        if isinstance(feature_probability, float):\n",
        "            feature_probability = t.tensor(feature_probability)\n",
        "        self.feature_probability = feature_probability.to(device).broadcast_to(\n",
        "            (cfg.n_inst, cfg.n_features)\n",
        "        )\n",
        "        if isinstance(importance, float):\n",
        "            importance = t.tensor(importance)\n",
        "        self.importance = importance.to(device).broadcast_to((cfg.n_inst, cfg.n_features))\n",
        "\n",
        "        self.W = nn.Parameter(\n",
        "            nn.init.xavier_normal_(t.empty((cfg.n_inst, cfg.d_hidden, cfg.n_features)))\n",
        "        )\n",
        "        self.b_final = nn.Parameter(t.zeros((cfg.n_inst, cfg.n_features)))\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        features: Float[Tensor, \"... inst feats\"],\n",
        "    ) -> Float[Tensor, \"... inst feats\"]:\n",
        "        \"\"\"\n",
        "        Performs a single forward pass. For a single instance, this is given by:\n",
        "            x -> ReLU(W.T @ W @ x + b_final)\n",
        "        \"\"\"\n",
        "        h = einops.einsum(\n",
        "            features, self.W, \"... inst feats, inst hidden feats -> ... inst hidden\"\n",
        "        )\n",
        "        out = einops.einsum(\n",
        "            h, self.W, \"... inst hidden, inst hidden feats -> ... inst feats\"\n",
        "        )\n",
        "        return F.relu(out + self.b_final)\n",
        "\n",
        "    def generate_batch(self, batch_size: int) -> Float[Tensor, \"batch inst feats\"]:\n",
        "        \"\"\"\n",
        "        Generates a batch of data of shape (batch_size, n_instances, n_features).\n",
        "        \"\"\"\n",
        "        batch_shape = (batch_size, self.cfg.n_inst, self.cfg.n_features)\n",
        "        feat_mag = t.rand(batch_shape, device=self.W.device)\n",
        "        feat_seeds = t.rand(batch_shape, device=self.W.device)\n",
        "        return t.where(feat_seeds <= self.feature_probability, feat_mag, 0.0)\n",
        "\n",
        "    def calculate_loss(\n",
        "        self,\n",
        "        out: Float[Tensor, \"batch inst feats\"],\n",
        "        batch: Float[Tensor, \"batch inst feats\"],\n",
        "    ) -> Float[Tensor, \"\"]:\n",
        "        \"\"\"\n",
        "        Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n",
        "\n",
        "            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
        "\n",
        "        Remember, `self.importance` will always have shape (n_inst, n_features).\n",
        "        \"\"\"\n",
        "        error = self.importance * ((batch - out) ** 2)\n",
        "        loss = einops.reduce(error, \"batch inst feats -> inst\", \"mean\").sum()\n",
        "        return loss\n",
        "\n",
        "    def optimize(\n",
        "        self,\n",
        "        batch_size: int = 1024,\n",
        "        steps: int = 5_000,\n",
        "        log_freq: int = 50,\n",
        "        lr: float = 1e-3,\n",
        "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Optimizes the model using the given hyperparameters.\n",
        "        \"\"\"\n",
        "        optimizer = t.optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        progress_bar = tqdm(range(steps))\n",
        "\n",
        "        for step in progress_bar:\n",
        "            # Update learning rate\n",
        "            step_lr = lr * lr_scale(step, steps)\n",
        "            for group in optimizer.param_groups:\n",
        "                group[\"lr\"] = step_lr\n",
        "\n",
        "            # Optimize\n",
        "            optimizer.zero_grad()\n",
        "            batch = self.generate_batch(batch_size)\n",
        "            out = self(batch)\n",
        "            loss = self.calculate_loss(out, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Display progress bar\n",
        "            if step % log_freq == 0 or (step + 1 == steps):\n",
        "                progress_bar.set_postfix(loss=loss.item() / self.cfg.n_inst, lr=step_lr)\n"
      ],
      "metadata": {
        "id": "kdnBL-HhL4Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOoGsDPjDfFO"
      },
      "source": [
        "### Notation\n",
        "\n",
        "The autoencoder's hidden activations go by many names. Sometimes they're called **neurons** (since they do have an activation function applied to them which makes them a privileged basis, like the neurons in an MLP layer). Sometimes they're called **features**, since the idea with SAEs is that these hidden activations are meant to refer to specific features in the data. However, the word feature is a bit [overloaded](https://www.lesswrong.com/posts/9Nkb389gidsozY9Tf/lewis-smith-s-shortform#fd64ALuWK8rXdLKz6) - ideally we want to use \"feature\" to refer to the attributes of the data itself - if our SAE's weights are randomly initialized, is it fair to call this a feature?!\n",
        "\n",
        "For this reason, we'll be referring to the autoencoder's hidden activations as **SAE latents**. However, it's worth noting that people sometimes use \"SAE features\" or \"neurons\" instead, so try not to get confused (e.g. often people use \"neuron resampling\" to refer to the resampling of the weights in the SAE).\n",
        "\n",
        "The new notation we'll adopt in this section is:\n",
        "\n",
        "- `d_sae`, which is the number of activations in the SAE's hidden layer (i.e. the latent dimension). Note that we want the SAE latents to correspond to the original data features, which is why we'll need `d_sae >= n_features` (usually we'll have equality in this section).\n",
        "- `d_in`, which is the SAE input dimension. This is the same as `d_hidden` from the previous sections because the SAE is reconstructing the model's hidden activations, however calling it `d_hidden` in the context of an SAE would be confusing. Usually in this section, we'll have `d_in = d_hidden = 2`, so we can visualize the results.\n",
        "\n",
        "<details>\n",
        "<summary>Question - in the formulas above (in the \"Problem setup\" section), what are the shapes of x, x', z, h, and h' ?</summary>\n",
        "\n",
        "Ignoring batch and instance dimensions:\n",
        "\n",
        "- `x` and `x'` are vectors of shape `(n_features,)`\n",
        "- `z` is a vector of shape `(d_sae,)`\n",
        "- `h` and `h'` are vectors of shape `(d_in,)`, which is equal to `d_hidden` from previous sections\n",
        "\n",
        "Including batch and instance dimensions, all shapes have extra leading dimensions `(batch_size, n_inst, d)`.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH7bzJtZDfFO"
      },
      "source": [
        "### SAE class\n",
        "\n",
        "We've provided the `ToySAEConfig` class below. Its arguments are as follows:\n",
        "\n",
        "- `n_inst`, which means the same as it does in your `ToyModel` class\n",
        "- `d_in`, the input size to your SAE (equal to `d_hidden` of your `ToyModel` class)\n",
        "- `d_sae`, the SAE's latent dimension size\n",
        "- `sparsity_coeff`, which is used in your loss function\n",
        "- `weight_normalize_eps`, which is added to the denominator whenever you normalize weights\n",
        "- `tied_weights`, which is a boolean determining whether your encoder and decoder weights are tied\n",
        "\n",
        "We've also given you the `ToySAE` class. Your job over the next exercise will be to fill in the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHX2jI-eDfFO"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ToySAEConfig:\n",
        "    n_inst: int\n",
        "    d_in: int\n",
        "    d_sae: int\n",
        "    sparsity_coeff: float = 0.2\n",
        "    weight_normalize_eps: float = 1e-8\n",
        "    tied_weights: bool = False\n",
        "\n",
        "\n",
        "class ToySAE(nn.Module):\n",
        "    W_enc: Float[Tensor, \"inst d_in d_sae\"]\n",
        "    _W_dec: Float[Tensor, \"inst d_sae d_in\"] | None\n",
        "    b_enc: Float[Tensor, \"inst d_sae\"]\n",
        "    b_dec: Float[Tensor, \"inst d_in\"]\n",
        "\n",
        "    def __init__(self, cfg: ToySAEConfig, model: ToyModel) -> None:\n",
        "        super(ToySAE, self).__init__()\n",
        "\n",
        "        assert cfg.d_in == model.cfg.d_hidden, \"Model's hidden dim doesn't match SAE input dim\"\n",
        "        self.cfg = cfg\n",
        "        self.model = model.requires_grad_(False)\n",
        "        # Setting up all toy models as exactly the same.\n",
        "        self.model.W.data[1:] = self.model.W.data[0]\n",
        "        self.model.b_final.data[1:] = self.model.b_final.data[0]\n",
        "\n",
        "        self.W_enc = nn.Parameter(nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_in, cfg.d_sae))))\n",
        "        self._W_dec = (\n",
        "            None\n",
        "            if self.cfg.tied_weights\n",
        "            else nn.Parameter(nn.init.kaiming_uniform_(t.empty((cfg.n_inst, cfg.d_sae, cfg.d_in))))\n",
        "        )\n",
        "        self.b_enc = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_sae))\n",
        "        self.b_dec = nn.Parameter(t.zeros(cfg.n_inst, cfg.d_in))\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    @property\n",
        "    def W_dec(self) -> Float[Tensor, \"inst d_sae d_in\"]:\n",
        "        return self._W_dec if self._W_dec is not None else self.W_enc.transpose(-1, -2)\n",
        "\n",
        "    @property\n",
        "    def W_dec_normalized(self) -> Float[Tensor, \"inst d_sae d_in\"]:\n",
        "        \"\"\"Returns decoder weights, normalized over the autoencoder input dimension.\"\"\"\n",
        "        return self.W_dec / (self.W_dec.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps)\n",
        "\n",
        "    def generate_batch(self, batch_size: int) -> Float[Tensor, \"batch inst d_in\"]:\n",
        "        \"\"\"\n",
        "        Generates a batch of hidden activations from our model.\n",
        "        \"\"\"\n",
        "        return einops.einsum(\n",
        "            self.model.generate_batch(batch_size),\n",
        "            self.model.W,\n",
        "            \"batch inst feats, inst d_in feats -> batch inst d_in\",\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, h: Float[Tensor, \"batch inst d_in\"]\n",
        "    ) -> tuple[\n",
        "        dict[str, Float[Tensor, \"batch inst\"]],\n",
        "        Float[Tensor, \"batch inst\"],\n",
        "        Float[Tensor, \"batch inst d_sae\"],\n",
        "        Float[Tensor, \"batch inst d_in\"],\n",
        "    ]:\n",
        "        \"\"\"\n",
        "        Forward pass on the autoencoder.\n",
        "\n",
        "        Args:\n",
        "            h: hidden layer activations of model\n",
        "\n",
        "        Returns:\n",
        "            loss_dict:       dict of different loss terms, each having shape (batch_size, n_inst)\n",
        "            loss:            total loss (i.e. sum over terms of loss dict), same shape as loss terms\n",
        "            acts_post:       autoencoder latent activations, after applying ReLU\n",
        "            h_reconstructed: reconstructed autoencoder input\n",
        "        \"\"\"\n",
        "        # You'll fill this in later\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def optimize(\n",
        "        self,\n",
        "        batch_size: int = 1024,\n",
        "        steps: int = 10_000,\n",
        "        log_freq: int = 100,\n",
        "        lr: float = 1e-3,\n",
        "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
        "        resample_method: Literal[\"simple\", None] = None,\n",
        "        resample_freq: int = 2500,\n",
        "        resample_window: int = 500,\n",
        "        resample_scale: float = 0.5,\n",
        "        hidden_sample_size: int = 256,\n",
        "    ) -> list[dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Optimizes the autoencoder using the given hyperparameters.\n",
        "\n",
        "        Args:\n",
        "            model:              we reconstruct features from model's hidden activations\n",
        "            batch_size:         size of batches we pass through model & train autoencoder on\n",
        "            steps:              number of optimization steps\n",
        "            log_freq:           number of optimization steps between logging\n",
        "            lr:                 learning rate\n",
        "            lr_scale:           learning rate scaling function\n",
        "            resample_method:    method for resampling dead latents\n",
        "            resample_freq:      number of optimization steps between resampling dead latents\n",
        "            resample_window:    number of steps needed for us to classify a neuron as dead\n",
        "            resample_scale:     scale factor for resampled neurons\n",
        "            hidden_sample_size: size of hidden value sample we add to the logs (for visualization)\n",
        "\n",
        "        Returns:\n",
        "            data_log:           dictionary containing data we'll use for visualization\n",
        "        \"\"\"\n",
        "        assert resample_window <= resample_freq\n",
        "\n",
        "        optimizer = t.optim.Adam(self.parameters(), lr=lr)  # betas=(0.0, 0.999)\n",
        "        frac_active_list = []\n",
        "        progress_bar = tqdm(range(steps))\n",
        "\n",
        "        # Create lists of dicts to store data we'll eventually be plotting\n",
        "        data_log = []\n",
        "\n",
        "        for step in progress_bar:\n",
        "            # Resample dead latents\n",
        "            if (resample_method is not None) and ((step + 1) % resample_freq == 0):\n",
        "                frac_active_in_window = t.stack(frac_active_list[-resample_window:], dim=0)\n",
        "                if resample_method == \"simple\":\n",
        "                    self.resample_simple(frac_active_in_window, resample_scale)\n",
        "\n",
        "            # Update learning rate\n",
        "            step_lr = lr * lr_scale(step, steps)\n",
        "            for group in optimizer.param_groups:\n",
        "                group[\"lr\"] = step_lr\n",
        "\n",
        "            # Get a batch of hidden activations from the model\n",
        "            with t.inference_mode():\n",
        "                h = self.generate_batch(batch_size)\n",
        "\n",
        "            # Optimize\n",
        "            loss_dict, loss, acts, _ = self.forward(h)\n",
        "            loss.mean(0).sum().backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Normalize decoder weights by modifying them directly (if not using tied weights)\n",
        "            if not self.cfg.tied_weights:\n",
        "                self.W_dec.data = self.W_dec_normalized.data\n",
        "\n",
        "            # Calculate the mean sparsities over batch dim for each feature\n",
        "            frac_active = (acts.abs() > 1e-8).float().mean(0)\n",
        "            frac_active_list.append(frac_active)\n",
        "\n",
        "            # Display progress bar, and log a bunch of values for creating plots / animations\n",
        "            if step % log_freq == 0 or (step + 1 == steps):\n",
        "                progress_bar.set_postfix(\n",
        "                    lr=step_lr,\n",
        "                    loss=loss.mean(0).sum().item(),\n",
        "                    frac_active=frac_active.mean().item(),\n",
        "                    **{k: v.mean(0).sum().item() for k, v in loss_dict.items()},  # type: ignore\n",
        "                )\n",
        "                with t.inference_mode():\n",
        "                    loss_dict, loss, acts, h_r = self.forward(\n",
        "                        h := self.generate_batch(hidden_sample_size)\n",
        "                    )\n",
        "                data_log.append(\n",
        "                    {\n",
        "                        \"steps\": step,\n",
        "                        \"frac_active\": (acts.abs() > 1e-8).float().mean(0).detach().cpu(),\n",
        "                        \"loss\": loss.detach().cpu(),\n",
        "                        \"h\": h.detach().cpu(),\n",
        "                        \"h_r\": h_r.detach().cpu(),\n",
        "                        **{name: param.detach().cpu() for name, param in self.named_parameters()},\n",
        "                        **{name: loss_term.detach().cpu() for name, loss_term in loss_dict.items()},\n",
        "                    }\n",
        "                )\n",
        "\n",
        "        return data_log\n",
        "\n",
        "    @t.no_grad()\n",
        "    def resample_simple(\n",
        "        self,\n",
        "        frac_active_in_window: Float[Tensor, \"window inst d_sae\"],\n",
        "        resample_scale: float,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Resamples dead latents, by modifying the model's weights and biases inplace.\n",
        "\n",
        "        Resampling method is:\n",
        "            - For each dead neuron, generate a random vector of size (d_in,), and normalize these vecs\n",
        "            - Set new values of W_dec and W_enc to be these normalized vectors, at each dead neuron\n",
        "            - Set b_enc to be zero, at each dead neuron\n",
        "\n",
        "        This function performs resampling over all instances at once, using batched operations.\n",
        "        \"\"\"\n",
        "        # Get a tensor of dead latents\n",
        "        dead_latents_mask = (frac_active_in_window < 1e-8).all(dim=0)  # [instances d_sae]\n",
        "        n_dead = int(dead_latents_mask.int().sum().item())\n",
        "\n",
        "        # Get our random replacement values of shape [n_dead d_in], and scale them\n",
        "        replacement_values = t.randn((n_dead, self.cfg.d_in), device=self.W_enc.device)\n",
        "        replacement_values_normed = replacement_values / (\n",
        "            replacement_values.norm(dim=-1, keepdim=True) + self.cfg.weight_normalize_eps\n",
        "        )\n",
        "\n",
        "        # Change the corresponding values in W_enc, W_dec, and b_enc\n",
        "        self.W_enc.data.transpose(-1, -2)[dead_latents_mask] = (\n",
        "            resample_scale * replacement_values_normed\n",
        "        )\n",
        "        self.W_dec.data[dead_latents_mask] = replacement_values_normed\n",
        "        self.b_enc.data[dead_latents_mask] = 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAsmViWFDfFO"
      },
      "source": [
        "<details>\n",
        "<summary>Why might we want / not want to tie our weights?</summary>\n",
        "\n",
        "In our `Model` implementations, we used a weight and its transpose. You might think it also makes sense to have the encoder and decoder weights be transposed copies of each other, since intuitively both the encoder and decoder's latent vectors meant to represent some feature's \"direction in the original model's hidden dimension\".\n",
        "\n",
        "The reason we might not want to tie weights is pretty subtle. The job of the encoder is in some sense to recover features from superposition, whereas the job of the decoder is just to represent that feature faithfully if present (since the goal of our SAE is to write the input as a linear combination of `W_dec` vectors) - this is why we generally see the decoder weights as the \"true direction\" for a feature, when weights are untied.\n",
        "\n",
        "The diagram below might help illustrate this concept (if you want, you can replicate the results in this diagram using our toy model setup!).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/w-dec-explained.png\" width=\"700\">\n",
        "\n",
        "In simple settings like this toy model we might not benefit much from untying weights, and tying weights can actually help us avoid finding annoying local minima in our optimization. However, for most of these exercises we'll use untied weights in order to illustrate SAE concepts more clearly.\n",
        "\n",
        "</details>\n",
        "\n",
        "Also, note that we've defined `self.cfg` and `self.model` for you in the init function - in the latter case, we've frozen the model's weights (because when you train your SAE you don't want to track gradients in your base model), and we've also modified the model's weights so they all match the first instance (this is so we can more easily interpret our SAE plots we'll create when we finish training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmLsc1rODfFP"
      },
      "source": [
        "### Exercise - implement `forward`\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵🔵\n",
        ">\n",
        "> You should spend up to 25-40 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "You should calculate the autoencoder's hidden state activations as $z = \\operatorname{ReLU}(W_{enc}(h - b_{dec}) + b_{enc})$, and then reconstruct the output as $h' = W_{dec}z + b_{dec}$. A few notes:\n",
        "\n",
        "- The **first variable** we return is a `loss_dict`, which contains the loss tensors of shape `(batch_size, n_inst)` for both terms in our loss function (before multiplying by the L1 coefficient). This is used for logging, and it'll also be used later in our neuron resampling methods. For this architecture, your keys should be `\"L_reconstruction\"` and `\"L_sparsity\"`.\n",
        "- The **second variable** we return is the `loss` term, which also has shape `(batch_size, n_inst)`, and is created by summing the losses in `loss_dict` (with sparsity loss multiplied by `cfg.sparsity_coeff`). When doing gradient descent, we'll average over the batch dimension & sum over the instance dimension (since we're training our instances independently & in parallel).\n",
        "- The **third variable** we return is the hidden state activations `acts`, which are also used later for neuron resampling (as well as logging how many latents are active).\n",
        "- The **fourth variable** we return is the reconstructed hidden states `h_reconstructed`, i.e. the autoencoder's actual output.\n",
        "\n",
        "An important note regarding our loss term - the reconstruction loss is the squared difference between input & output **averaged** over the `d_in` dimension, but the sparsity penalty is the L1 norm of the hidden activations **summed** over the `d_sae` dimension. Can you see why we average one but sum the other?\n",
        "\n",
        "<details>\n",
        "<summary>Hint</summary>\n",
        "\n",
        "Suppose we averaged L1 loss too. Consider the gradients a single latent receives from the reconstruction loss and sparsity penalty - what do they look like in the limit of very large `d_sae`?\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Answer - why we average L2 loss over <code>d_in</code> but sum L1 loss over <code>d_sae</code></summary>\n",
        "\n",
        "Suppose for sake of argument we averaged L1 loss too. Imagine if we doubled the latent dimension, but kept all other SAE hyperparameters the same. The per-hidden-unit gradient from the reconstruction loss would still be the same (because changing a single hidden unit's encoder or decoder vector would have the same effect on the output as before), but the per-hidden-unit gradient from the sparsity penalty would have halved (because we're averaging the sparsity penalty over `d_sae`). This means that in the limit, the sparsity penalty wouldn't matter at all, and the only important thing would be getting zero reconstruction loss.\n",
        "\n",
        "</details>\n",
        "\n",
        "Note - make sure you're using `self.W_dec_normalized` rather than `self.W_dec` in your forward function. This is because if we're using tied weights then we won't be able to manually normalize `W_dec` inplace, but we still want to use the normalized version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1qGhDycDfFP"
      },
      "outputs": [],
      "source": [
        "# Go back up and edit your `ToySAE.forward` method, then run the test below\n",
        "\n",
        "tests.test_sae_forward(ToySAE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6By91OVDfFP"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def forward(\n",
        "    self: ToySAE, h: Float[Tensor, \"batch inst d_in\"]\n",
        ") -> tuple[\n",
        "    dict[str, Float[Tensor, \"batch inst\"]],\n",
        "    Float[Tensor, \"batch inst\"],\n",
        "    Float[Tensor, \"batch inst d_sae\"],\n",
        "    Float[Tensor, \"batch inst d_in\"],\n",
        "]:\n",
        "    \"\"\"\n",
        "    Forward pass on the autoencoder.\n",
        "\n",
        "    Args:\n",
        "        h: hidden layer activations of model\n",
        "\n",
        "    Returns:\n",
        "        loss_dict:       dict of different loss terms, each dict value having shape (batch_size, n_inst)\n",
        "        loss:            total loss (i.e. sum over terms of loss dict), same shape as loss_dict values\n",
        "        acts_post:       autoencoder latent activations, after applying ReLU\n",
        "        h_reconstructed: reconstructed autoencoder input\n",
        "    \"\"\"\n",
        "    h_cent = h - self.b_dec\n",
        "\n",
        "    # Compute latent (hidden layer) activations\n",
        "    acts_pre = (\n",
        "        einops.einsum(h_cent, self.W_enc, \"batch inst d_in, inst d_in d_sae -> batch inst d_sae\")\n",
        "        + self.b_enc\n",
        "    )\n",
        "    acts_post = F.relu(acts_pre)\n",
        "\n",
        "    # Compute reconstructed input\n",
        "    h_reconstructed = (\n",
        "        einops.einsum(\n",
        "            acts_post, self.W_dec_normalized, \"batch inst d_sae, inst d_sae d_in -> batch inst d_in\"\n",
        "        )\n",
        "        + self.b_dec\n",
        "    )\n",
        "\n",
        "    # Compute loss terms\n",
        "    L_reconstruction = (h_reconstructed - h).pow(2).mean(-1)\n",
        "    L_sparsity = acts_post.abs().sum(-1)\n",
        "    loss_dict = {\"L_reconstruction\": L_reconstruction, \"L_sparsity\": L_sparsity}\n",
        "    loss = L_reconstruction + self.cfg.sparsity_coeff * L_sparsity\n",
        "\n",
        "    return loss_dict, loss, acts_post, h_reconstructed\n",
        "\n",
        "\n",
        "ToySAE.forward = forward\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6tmn7jEDfFP"
      },
      "source": [
        "## Training your SAE\n",
        "\n",
        "The `optimize` method has been given to you. A few notes on how it differs from your previous model:\n",
        "\n",
        "- Before each optimization step, we implement **neuron resampling** - we'll get to this later.\n",
        "- We have more logging, via the `data_log` dictionary - we'll use this for visualization.\n",
        "- We've used `betas=(0.0, 0.999)`, to match the description in [Anthropic's Feb 2024 update](https://transformer-circuits.pub/2024/feb-update/index.html#dict-learning-loss) - although they document it to work better specifically for large models, we may as well match it here.\n",
        "\n",
        "First, let's define and train our model, and visualize model weights and the data returned from `sae.generate_batch` (which are the hidden state representations of our trained model, and will be used for training our SAE).\n",
        "\n",
        "Note that we'll use a feature probability of 2.5% for all subsequent exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqFErsRvDfFP"
      },
      "outputs": [],
      "source": [
        "d_hidden = d_in = 2\n",
        "n_features = d_sae = 5\n",
        "n_inst = 16\n",
        "\n",
        "# Create a toy model, and train it to convergence\n",
        "cfg = ToyModelConfig(n_inst=n_inst, n_features=n_features, d_hidden=d_hidden)\n",
        "model = ToyModel(cfg=cfg, device=device, feature_probability=0.025)\n",
        "model.optimize()\n",
        "\n",
        "sae = ToySAE(cfg=ToySAEConfig(n_inst=n_inst, d_in=d_in, d_sae=d_sae), model=model)\n",
        "\n",
        "h = sae.generate_batch(512)\n",
        "\n",
        "utils.plot_features_in_2d(model.W[:8], title=\"Base model\")\n",
        "utils.plot_features_in_2d(\n",
        "    einops.rearrange(h[:, :8], \"batch inst d_in -> inst d_in batch\"),\n",
        "    title=\"Hidden state representation of a random batch of data\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D_wGdjVDfFP"
      },
      "source": [
        "Now, let's train our SAE, and visualize the instances with lowest loss! We've also created a function `animate_features_in_2d` which creates an animation of the training over time. If the inline displaying doesn't work, you might have to open the saved HTML file in your browser to see it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W1nZ3quDfFP"
      },
      "outputs": [],
      "source": [
        "data_log = sae.optimize(steps=20_000, resample_method=None)\n",
        "\n",
        "utils.animate_features_in_2d(\n",
        "    data_log,\n",
        "    instances=list(range(8)),  # only plot the first 8 instances\n",
        "    rows=[\"W_enc\", \"_W_dec\"],\n",
        "    filename=str(section_dir / \"animation-training.html\"),\n",
        "    title=\"SAE on toy model\",\n",
        ")\n",
        "\n",
        "# If this display code doesn't work, try saving & opening animation in your browser\n",
        "with open(section_dir / \"animation-training.html\") as f:\n",
        "    display(HTML(f.read()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZG7r9QhDfFP"
      },
      "source": [
        "In other words, the autoencoder is generally successful at reconstructing the model's hidden states, and maybe sometimes it learns the fully monosemantic solution (one latent per feature), but more often it learns a combination of **polysemantic latents** and **dead latents** (which never activate). These are a big problem because they don't receive any gradients during training, so they're not a problem which fixes itself over time. You can check the presence of dead latents by graphing the feature probabilities over training, in the code below. You should find that:\n",
        "\n",
        "1. Some latents are dead for most or all of training (with \"fraction of datapoints active\" being zero),\n",
        "2. Some latents fire more frequently than the target feature prob of 2.5% (these are usually polysemantic, i.e. they fire on more than one different feature),\n",
        "3. Some latents fire approximately at or slightly below the target probability (these are usually monosemantic). If any of your instances above learned the full monosemantic solution (i.e. latents uniformly spaced around the 2D hidden dimension) then you should find that all 5 latents in that instance fall into this third category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljIgTeCjDfFP"
      },
      "outputs": [],
      "source": [
        "utils.frac_active_line_plot(\n",
        "    frac_active=t.stack([data[\"frac_active\"] for data in data_log]),\n",
        "    title=\"Probability of sae features being active during training\",\n",
        "    avg_window=20,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd0W2JCsDfFP"
      },
      "source": [
        "## Resampling\n",
        "\n",
        "From Anthropic's paper (replacing terminology \"dead neurons\" with \"dead latents\" in accordance with how we're using the term):\n",
        "\n",
        "> Second, we found that over the course of training some latents cease to activate, even across a large number of datapoints. We found that “resampling” these dead latents during training gave better results by allowing the model to represent more features for a given autoencoder hidden layer dimension. Our resampling procedure is detailed in [Autoencoder Resampling](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-resampling), but in brief we periodically check for latents which have not fired in a significant number of steps and reset the encoder weights on the dead latents to match data points that the autoencoder does not currently represent well.\n",
        "\n",
        "We've implemented this procedure for you. Let's see the results!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtckL2z8DfFQ"
      },
      "source": [
        "Let's train your model again, and watch the animation to see how the neuron resampling has helped the training process. You should be able to see the resampled neurons in red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1fmS-RoDfFQ"
      },
      "outputs": [],
      "source": [
        "resampling_sae = ToySAE(cfg=ToySAEConfig(n_inst=n_inst, d_in=d_in, d_sae=d_sae), model=model)\n",
        "\n",
        "resampling_data_log = resampling_sae.optimize(steps=20_000, resample_method=\"simple\")\n",
        "\n",
        "utils.animate_features_in_2d(\n",
        "    resampling_data_log,\n",
        "    rows=[\"W_enc\", \"_W_dec\"],\n",
        "    instances=list(range(8)),  # only plot the first 8 instances\n",
        "    filename=str(section_dir / \"animation-training-resampling.html\"),\n",
        "    color_resampled_latents=True,\n",
        "    title=\"SAE on toy model (with resampling)\",\n",
        ")\n",
        "\n",
        "# If this display code doesn't work, try saving & opening animation in your browser\n",
        "with open(section_dir / \"animation-training-resampling.html\") as f:\n",
        "    display(HTML(f.read()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "utils.frac_active_line_plot(\n",
        "    frac_active=t.stack([data[\"frac_active\"] for data in resampling_data_log]),\n",
        "    title=\"Probability of sae features being active during training\",\n",
        "    avg_window=20,\n",
        ")"
      ],
      "metadata": {
        "id": "NTMdkPnMXdoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkN7qArDfFQ"
      },
      "source": [
        "Much better!\n",
        "\n",
        "Now that we have pretty much full reconstruction on our features, let's visualize that reconstruction! The `animate_features_in_2d` function also offers features to plot hidden state reconstructions and how they evolve over time. Examining how the hidden state reconstructions evolve over time can help you understand what's going on, for example:\n",
        "\n",
        "- The SAE often learns a non-sparse solution (e.g. 4 uniformly spaced polysemantic latents & 1 dead latent) before converging to the ideal solution.\n",
        "    - Note, we also see something similar when training SAEs on LLMs: they first find a non-sparse solution with small reconstruction loss, before learning a more sparse solution (L0 goes down).\n",
        "- Hovering over hidden states, you should observe some things:\n",
        "    - Low-magnitude hidden states are often reconstructed as zero, this is because the SAE can't separate them from interference from other features.\n",
        "    - Even for correctly reconstructed features, the hidden state magnitude is generally smaller than the true hidden states - this is called **shrinkage**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPXEKl04DfFQ"
      },
      "outputs": [],
      "source": [
        "utils.animate_features_in_2d(\n",
        "    resampling_data_log,\n",
        "    rows=[\"W_enc\", \"h\", \"h_r\"],\n",
        "    instances=list(range(4)),  # plotting fewer instances for a smaller animation file size\n",
        "    color_resampled_latents=True,\n",
        "    filename=str(section_dir / \"animation-training-reconstructions.html\"),\n",
        "    title=\"SAE on toy model (showing hidden states & reconstructions)\",\n",
        ")\n",
        "\n",
        "# If this display code doesn't work, try saving & opening animation in your browser\n",
        "with open(section_dir / \"animation-training-reconstructions.html\") as f:\n",
        "    display(HTML(f.read()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0s73-DiDfFQ"
      },
      "source": [
        "**That's all for the tutorial! Thank you for your attention!**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}